{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cykj-q7h5bM"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcF8cgWTh1B3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_INDEX=4\n",
    "isGPU = True\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "NUM_EPOCHES = 50\n",
    "NUM_CLASS = 100\n",
    "EPOCH_THRES=5\n",
    "MODEL_NAME = \"resnet\"\n",
    "DATA_NAME=\"cifar\"\n",
    "DATA_DIR = f'./data/{DATA_NAME}-{NUM_CLASS}'\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "thresholds = [[0.1], [0.2],[0.3], [0.4], [0.5], [0.6], [0.7],[0.8], [0.9]]\n",
    "to_keep = [[False, True]]*len(thresholds)\n",
    "\n",
    "\n",
    "# thresholds = [[0.1, 0.9], [0.2, 0.8], [0.3, 0.7]]\n",
    "# to_keep = [[False, True, True] , [False, True, True], [False, True, True]]\n",
    "\n",
    "# thresholds = [[0.1, 0.3, 0.7], [0.2, 0.5, 0.8]]\n",
    "# to_keep = [[False, True, True, False], [False, True, True, False]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # HF model\n",
    "# HF_API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "\n",
    "if isGPU:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" ## to avoid Context Switching \n",
    "    os.environ[\"HF_HOME\"]= \"/data2/meithnav/.hfcache/\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(GPU_INDEX) # not changing GPU. Only \n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(GPU_INDEX) # not changing GPU. Only \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if isGPU:\n",
    "    torch.cuda.set_device(0) ## setgpu\n",
    "    print(\"\\n\\n--> CONNECTED TO GPU NO: \", torch.cuda.current_device())\n",
    "    print(\"--> GPU_INDEX: \", GPU_INDEX)\n",
    "        \n",
    "    # GPU (MPS for Apple Silicon, CUDA for Nvidia GPUs, or CPU)\n",
    "\n",
    "    torch.cuda.empty_cache() # clear GPU cache\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# APPEND ROOT DIRECTORY\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('hangman'), '..')))\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "    \n",
    "\n",
    "if not os.path.exists('./outputs'):\n",
    "    os.makedirs('./outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvvtnU1eh8ak"
   },
   "outputs": [],
   "source": [
    "class ForgetabilityTracker:\n",
    "    def __init__(self, dataset_size, device):\n",
    "        self.misclassification_counts = np.zeros(dataset_size, dtype=np.int32)\n",
    "\n",
    "    def update(self, predictions, labels, indices):\n",
    "        incorrect_predictions = predictions != labels\n",
    "        incorrect_predictions = incorrect_predictions.cpu().numpy()\n",
    "        indices = indices.cpu().numpy()\n",
    "        self.misclassification_counts[indices] += incorrect_predictions\n",
    "\n",
    "    def get_scores(self):\n",
    "        return self.misclassification_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFawbq8tio01"
   },
   "source": [
    "ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnw0VXKOiFzc"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Skip connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASS):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, NUM_CLASS)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpB1ujCymzFQ"
   },
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    train_loader, \n",
    "    tracker=None, \n",
    "    epoch_threshold=5, \n",
    "    thresholds_arr=None, \n",
    "    to_keep=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a model with dynamic dataset adjustment based on forgetability and thresholds.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        optimizer: Optimizer for training.\n",
    "        criterion: Loss function.\n",
    "        train_loader: DataLoader for the training dataset.\n",
    "        tracker: ForgetabilityTracker instance, optional.\n",
    "        epoch_threshold: Number of epochs after which to update the dataset.\n",
    "        thresholds_arr: Threshold values (float, tuple, or list of thresholds).\n",
    "        to_keep: List of booleans indicating which bins to retain.\n",
    "    \"\"\"\n",
    "    current_loader = train_loader  # Use the initial loader for the first phase\n",
    "\n",
    "    for epoch in range(NUM_EPOCHES):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, targets) in tqdm(enumerate(current_loader), desc=f\"Epoch {epoch + 1}/{NUM_EPOCHES}\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if tracker:\n",
    "                indices = batch_idx * train_loader.batch_size + torch.arange(data.size(0)).to(device)\n",
    "                tracker.update(predictions, targets, indices)\n",
    "\n",
    "        # Update dataset every `epoch_threshold` epochs if tracker is enabled\n",
    "        if tracker and thresholds_arr and (epoch + 1) % epoch_threshold == 0:\n",
    "            print(f\"Epoch {epoch + 1}: Evaluating forgetability and updating dataset...\")\n",
    "\n",
    "            # Get forgetability scores and normalize\n",
    "            forgetability_scores = tracker.get_scores()\n",
    "            normalized_scores = (forgetability_scores - np.min(forgetability_scores)) / \\\n",
    "                                (np.max(forgetability_scores) - np.min(forgetability_scores))\n",
    "\n",
    "            # Multi-threshold case: Create distinct bins\n",
    "            bin_indices = []\n",
    "            for i, threshold in enumerate(thresholds_arr):\n",
    "                if i == 0:\n",
    "                    bin_indices.append(np.where(normalized_scores <= threshold)[0])\n",
    "                else:\n",
    "                    bin_indices.append(\n",
    "                        np.where((normalized_scores > thresholds_arr[i - 1]) & (normalized_scores <= threshold))[0]\n",
    "                    )\n",
    "            bin_indices.append(np.where(normalized_scores > thresholds_arr[-1])[0])\n",
    "\n",
    "            # Filter bins based on `to_keep`\n",
    "            if to_keep is not None:\n",
    "                if len(to_keep) != len(bin_indices):\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid `to_keep` length. Expected {len(bin_indices)} booleans, but got {len(to_keep)}.\"\n",
    "                    )\n",
    "                bin_indices = [bin for bin, keep in zip(bin_indices, to_keep) if keep]\n",
    "\n",
    "            indices_to_keep = np.concatenate(bin_indices) if bin_indices else np.array([], dtype=int)\n",
    "            print(f\"Keeping {len(indices_to_keep)} out of {len(forgetability_scores)} datapoints.\")\n",
    "\n",
    "            # Update the training dataset\n",
    "            current_loader = DataLoader(\n",
    "                Subset(train_loader.dataset, indices_to_keep),\n",
    "                batch_size=train_loader.batch_size,\n",
    "                shuffle=True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5ctbO3Bm4I0"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LT2HGSYVm1bp"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8pamxH6nBI_"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DuUI82em5wm",
    "outputId": "1aafaa05-b340-4f90-9363-2f8b370732e4"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "if NUM_CLASS==10:\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transform)\n",
    "elif NUM_CLASS==100: \n",
    "    train_dataset = torchvision.datasets.CIFAR100(root=DATA_DIR, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root=DATA_DIR, train=False, download=True, transform=transform)\n",
    "else: \n",
    "    train_dataset=None\n",
    "    test_dataset=None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader_full = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0ugUtDvnFAh",
    "outputId": "72409cf7-24f7-4eca-cd62-45a17795a096"
   },
   "outputs": [],
   "source": [
    "# BASELINE: Train on the entire dataset\n",
    "\n",
    "model_baseline = ResNet18(num_classes=NUM_CLASS).to(device)\n",
    "optimizer_baseline = optim.Adam(model_baseline.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n\\n****\\nTraining baseline model...\")\n",
    "train_model(model_baseline, optimizer_baseline, criterion, train_loader_full)\n",
    "accuracy_baseline = evaluate_model(model_baseline, test_loader)\n",
    "print(f\"Baseline Model Accuracy: {accuracy_baseline * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing for thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ahz_C4b-ao7o"
   },
   "outputs": [],
   "source": [
    "train_loader_dynamic = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for idx, thres in tqdm(enumerate(thresholds)):\n",
    "  print(f\"\\n\\n****\\n-> RUNNING THRES : {thres}, MODEL: {MODEL_NAME}, DATASET: {DATA_NAME}-{NUM_CLASS}\")\n",
    "  model_dynamic = ResNet18().to(device)\n",
    "  optimizer_dynamic = optim.Adam(model_dynamic.parameters(), lr=0.001)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  tracker_dynamic = ForgetabilityTracker(len(train_dataset), device)\n",
    "\n",
    "  train_model(model_dynamic, optimizer_dynamic, criterion, train_loader_dynamic, tracker_dynamic, EPOCH_THRES, thres, to_keep[idx])\n",
    "\n",
    "  accuracy_dynamic = evaluate_model(model_dynamic, test_loader)\n",
    "  accuracies.append(accuracy_dynamic)\n",
    "  torch.save(model_dynamic, f'./models/{MODEL_NAME}-{DATA_NAME}-{NUM_CLASS}-thres-{thres[0]}.pt')\n",
    "  print(f\"-> TEST ACC : {accuracy_dynamic}, THRES : {thres}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8GeYt7vnPeL"
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, accuracies)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Threshold')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.savefig(f'./outputs/{MODEL_NAME}-{DATA_NAME}-{NUM_CLASS}-strategy-{ '-'.join(map(str, to_keep[0])) }.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "domianins",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
